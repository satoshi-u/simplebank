===============================================================================================================================================
#1 -> Schema Essentials

        drew the schema here and get simple-bank.pdf/simple-bank.sql : https://dbdiagram.io/d/6443ae366b3194705103b899

===============================================================================================================================================
#2 -> Docker Essentials

        start postgres docker container
        $ docker run --name simple-bank-db -p 5432:5432 -e POSTGRES_USER=root -e POSTGRES_PASSWORD=secret -d postgres:12-alpine

        run cmd inside container (pw not required because image sets up trust authentication locally)
        $ docker exec -it simple-bank-db psql -U root
        $ select now();

        get all containers
        $ docker ps -a

        get logs of container
        $ docker logs simple-bank-db

        stop container
        $ docker stop simple-bank-db

        remove container
        $ docker rm simple-bank-db

        restart container
        $ docker start simple-bank-db

        go inside container
        $ docker exec -it simple-bank-db bash
        $ docker exec -it simple-bank-db /bin/sh

        create a new db with psql cli from inside or directly
        $ createdb --username=root --owner=root simple_bank
        $ docker exec -it simple-bank-db createdb --username=root --owner=root simple_bank

        access new db with psql cli from inside or directly
        $ psql -U root simple_bank
        $ docker exec -it simple-bank-db psql -U root simple_bank

        delete new db with psql cli from inside or directly
        $ dropdb simple_bank
        $ docker exec -it simple-bank-db dropdb simple_bank

        makefile -> easy to setup in local for new folks in project

===============================================================================================================================================
#3 -> DB Migration Essentials

        Install cli golang-migrate
        $ brew install golang-migrate

        create <init_schema> named migration for project with flags
                -seq : uses a sequential number as migration filename's prefix
                -dir : output dir where migration files are saved
        $ migrate create -ext sql -dir db/migration -seq init_schema

        run migrate up/down (included in makefile)
        $ migrate -path db/migration -database "postgresql://root:secret@localhost:5432/simple_bank?sslmode=disable" -verbose up
        $ migrate -path db/migration -database "postgresql://root:secret@localhost:5432/simple_bank?sslmode=disable" -verbose down

===============================================================================================================================================
#4 -> Generate golang CRUD from SQL

        use for doing low-level custom queries : https://pkg.go.dev/database/sql
        use for doing high-level queries : https://pkg.go.dev/gorm.io/gorm
        faster alt : https://pkg.go.dev/github.com/jmoiron/sqlx
        faster, autogen & compile-time alt: https://pkg.go.dev/github.com/kyleconroy/sqlc

        Install sqlc
        $ brew install sqlc

        Init - genrates a yaml with empty settings. For more : https://github.com/kyleconroy/sqlc/tree/v1.4.0
        $ sqlc init

        Init go project and tidy project
        $ go mod init github.com/web3dev6/simplebank
        $ go mod tidy

        Generate code (included in makefile, add .sql in db/query and then run)
        $ sqlc generate

===============================================================================================================================================
#5 -> Golang unit tests for db CRUD

        note*
            $ go get github.com/lib/pq
            Add _ "github.com/lib/pq" in imports for the pgdb driver to work properly
            $ go mod tidy to fix go.mod dependency

        Matching test results in go tests
        $ go get github.com/stretchr/testify

        note*
            Added unit tests for account, entry and transfer db CRUD generated code, which use testQueries conn created in main_test 
        
        $ go test ./...

===============================================================================================================================================
#6 -> SQL DB Txn

        note*
            updated Store to have both Queries and sql.db both
            wrote a generic execTx to carry out SQL DB Txns
            used execTx to carry out multiple create/update queries in one single Tx
            wrote unit test(s) for store 

===============================================================================================================================================
#7 -> DB Txn Lock & dealing with Deadlocks

        note*
            deadlocks happen because of concurrent txns in db
            tx2 : {create transfer}[ok]
            tx2 : {create entry}[ok]
            tx1 : {create transfer}[holding-exclusive-lock]... LOCK HERE
            tx2 : {create entry}[ok]
            tx2 : {get account}[waiting-shared-lock]... WAITING LOCK
            tx1 : {create entry}[ok]
            tx1 : {create entry}[ok]
            tx1 : {get account}[waiting-shared-lock]... DEADLOCK

        note*
            one way to avoid deadlocks is by removing the foreign key constraints on tables[ACCOUNT<->TRANSFER]

            lock is required by "tx1 : {create transfer}" as postgres worries that other txs might update the transfer table's referred from-account-id/to-account-id in accounts table,
            thereby it must guard the foreign key constraint whilst updating the transfer record
            one potential solution is in query "select account for update",
            we must specify that pk won't be touched in "update account" query which is true also
            $ SELECT * FROM accounts WHERE id = $1 LIMIT 1 FOR NO KEY UPDATE;
            Regenerate autogen code sqlc with updated account.sql
            $ make sqlc

            optimize running 2 queries @account{get-update} by having a single query @account{update}

===============================================================================================================================================
#8 -> Avoid Deadlocks - keep order same

        note*
            Deadlock can still happen because of concurrent txns updating pair of same records in different order
            To avoid, make txns update the records in same order always

            Application must acquire locks in a consistent order

===============================================================================================================================================
#9 -> Isolation Levels & Read Phenomenon

        ACID -
                Atomicity   : Either all operations complete successfully or txn fails and db left unchanged
                Consistency : Db state must be valid after the tx, all constraints must be satisfied
                Isolation   : Concurrent txns must not affect each other
                Durability  : Data written by a successful tx must be recorded in persistent storage

        read Phenomenon: -
                Dirty Read            : A txn reads data written by other uncommitted tx
                Non Repeatable Read   : A txn reads the same row twice and sees different value as it got modified by other committed tx
                Phantom Read          : A txn reads a set of rows(based on some condition) twice and sees different values as it got modified by other committed tx
                Serialization Anomaly : The result of a group of concurrent committed txns is impossible to achieve if we run same txns sequentially(no overlaps) in any order
                                        case a. run concurrently 2 times (sum up balance of accounts in table and create new account with that sum as balance)
                                             b. run sequentially 2 times (sum up balance of accounts in table and create new account with that sum as balance)
                                             -> both give different results

        4 std isolation levels -
            Read Uncommitted  <->   doesn't even prevent Dirty Read
            Read Committed    <->   prevents Dirty Read, but Non Repeatable Read & Phantom Read can occur
            Repeatable Read   <->   prevents Dirty Read, Non Repeatable Read & Phantom Read but Serialization Anomaly can occur
            Serializable Read <->   prevents even Serialization Anomaly
        NOTE: rules only applicable for read queries, write is always in sync or throws error
        NOTE: you might wanna have txn retries in place when writes/updates are failed in Repeatable Read/Serializable Read
        NOTE: in postgres, read uncommitted behaves as read committed
        NOTE: in mysql, update to same entry is allowed to happen in concurrent txns with repeatable read mode
        NOTE: in postgres, update to same entry throws error in concurrent txns with repeatable read mode
        NOTE: in mysql's serializable read, serializable anamoly is prevented by throwing deadlock in 2nd txn in concurrent txns affecting same rows
        NOTE: in postgres's serializable read, serializable anamoly is prevented by throwing error in 2nd txn(tells to retry) in concurrent txns affecting same rows

        For an open mysql session,
            get mysql's current isolation level (default is repeatable read)
            $ select @transaction_isolation

            get mysql's global isolation level
            $ select @global.transaction_isolation

            mysql change isolation level
            $ set session transaction isolation level read uncommitted;
            $ set session transaction isolation level read committed;
            $ set session transaction isolation level repeatable read;
            $ set session transaction isolation level serializable read;
        
         For an open postgres session,
            get postgres's current isolation level (default is read committed)
            $ show transaction isolation level

            postgres change isolation level - at txn level
            $ begin
            $ set transaction isolation level read uncommitted;
            $ set transaction isolation level read committed;
            $ set transaction isolation level repeatable read;
            $ set transaction isolation level serializable read;

===============================================================================================================================================
#10 -> CI with Github Actions

        Create your yml file in .github/workflows for running go tests written till now
        For this, you must have the dependencies like golang, golang-migrate-binary, dockerized-postgres
        Then you checkout code, run migrateup and go build/test 
        
        github action for postgres reference
        https://docs.github.com/en/actions/using-containerized-services/creating-postgresql-service-containers

        golang migrate CLI here - find the tar for linux-amd in releases
        https://github.com/golang-migrate/migrate/tree/master/cmd/migrate

===============================================================================================================================================
#11 -> REST with Gin

        Install gin package
        $ go get -u github.com/gin-gonic/gin

        Gin validator package - for validating requests and picking data
        BODY_PARAMS  -> ctx.ShouldBind
        URI_PARAMS   -> ctx.ShouldBindUri
        QUERY_PARAMS -> ctx.ShouldBindQuery

        Gin send reponse -> use ctx.JSON()

        Gin return key-value pairs -> use gin.H (example: in errorResponse)

        TODO:   write api-routes for account(update, delete)

===============================================================================================================================================
#12 -> Viper for configuring project using env-vars/files

        Finds, loads, unmarshalls config from config file
        Supports JSON, YAML, ENV, TOML, INI
       
        Reads config from env-vars/flags
        Set default values, override existing values

        Reads config from remote system, works for both unencrypted and encrypted values 
        Supports Etc, Consul

        Watch for changes in the config file, and notifies application about it
        Re-read changed file, save any modifications made to config file

        Install viper package
        $ go get github.com/spf13/viper
       
        NOTE:   To unmarshall values, viper uses underneath -> $ go get github.com/mitchellh/mapstructure
                So use mapstructure tags to specify name of each config field

        TODO:   config live watching, reading config from remote system

===============================================================================================================================================
#13 -> Test REST APIs with MockDB

        Advantages:
                -> Independent tests : No conflicts with isolated tests data 
                -> Faster tests      : Reduce time spent in connecting/talking to a real db
                -> 100% coverage     : Easily setup edge cases - unexpected errors, connection lost, etc.

        Types of Mock DB:
                -> fake db(memory)   : Implement a fake version of db, store data in memory
                -> db stub(gomock)   : Generate and build stubs that return hard-coded values

        Install mockgo package -> gets bianry in $GOPATH/bin
        $ go install github.com/golang/mock/mockgen@v1.6.0

        To generate mock for Store Interface(lists the TransferTx & Querier methods), use mockgen @<go-module>/<path-to-interface> <interface>
        $ mockgen -destination db/mock/store.go -package mockdb github.com/web3dev6/simplebank/db/sqlc Store
                NOTE: use -destination flag to generate code in a file, default codegen goes to stdout
                NOTE: use -package flag to give a name of your choice for created package

        For API tests:
        Use *mockdb.MockStore created above, build stubs for Get API test-cases, and run these test-requests through a httptest/recorder (dummy server).
        Them evaluate the response as per test-case.

        TODO:   write API tests for all CRUD APIs on Account model

===============================================================================================================================================
#14 -> Transfer API with Custom validation on currency

        using Gin's custom validator:
        https://gin-gonic.com/docs/examples/custom-validators/ 
        which uses:
        https://github.com/go-playground/validator/

        NOTE:   Also wrote in server.go, before starting server, check if any accounts in db.Store 
                If not, create 10 accounts
                *(For this, added GetCountForAccounts in account.sql, and ran 
                        $ make sqlc - to add corresponding method in *Queries and also add in Querier interface while emitting
                        $ make mock - to add corresponding mock method in mockdb's mockStore
                )
        NOTE:  Also currencies-supported now in a central place -> in util.currency.go

        TODO:   write API tests for different scenarios in TransferTx 

===============================================================================================================================================
#15 -> Users table in DB with new migration

        Edit the diagram @ https://dbdiagram.io/d/6443ae366b3194705103b899
                -> to include the users table, and added a FK contraint between username in user-table to owner in account-table
                   now, a user can have multiple accounts
                -> also added a composite unique index for owner and currency in account-table so that a user can have only one account in one currency
                -> download the exported PostgreSQL file
        
        Create <add_user_schema> named migration for project with flags
                -seq : uses a sequential number as migration filename's prefix
                -dir : output dir where migration files are saved
        $ migrate create -ext sql -dir db/migration -seq add_user_schema

        Copy from downloaded .sql file to add_user_schema migration up .sql 
                -> the command to create users table and add FK/UNIQUE constraints
        Copy from downloaded .sql file to add_user_schema migration down .sql 
                -> the command remove FK/UNIQUE constraints, and drop users table

        In Makefile, add migrateup1 & migratedown1 to up/down 1 sequential migration at a time

===============================================================================================================================================
#16 -> sqlc generate for user.sql and handle Db errors in Go

        Run "make sqlc" to create user.sql.go which has create-user & get-user method on *Queries, thereby SQLStore
        Also, these methods were added in the Queries interface
        Now, also run "make mock" for mockgen to add these new methods in MockStore which mocks Store

        Write new user.sql.go db-tests in user_test.go (first write a createRandomUser func)
        Correct db/account.sql.go db-tests in db/account_test.go (must use createRandomUser to create a user, and then use that username as owner in creating acocunt, or else FK constraint fails tests)
        For now, api/account.go api-tests in api/account_test.go don't fail because of FK constraint, because there is no sql db, it's a mock store underneath

===============================================================================================================================================
#17 -> Hash password in Go with Bcrypt

        Added util/password.go and corresponding util/password_test.go
                -> uses golang's bcrypt to hash/check passwords
                -> used this util methods to create password in db/user_test.go and ran UTs successfully

        Added api/user.go to create & get user in users table with proper validations & error handling
        Note:   Hid hashedPassword in response by creating a new struct 

        Add pgx to your Go modules:
        $ go get github.com/jackc/pgx/v5
        Write db errors in a common db/error.go file, to be used in api-tests
                -> ErrRecordNotFound
                -> ErrUniqueViolation 

        Added unit tests for api/user.go in api/user_test.go 
                TestCreateUserAPI :  
                -> OK
                -> InternalError (sql.ErrConnDone)
                -> DuplicateUsername (db.ErrUniqueViolation)
                -> InvalidUsername
                -> InvalidEmail
                -> TooShortPassword
                Note:   func checkResponse :
                        can have the same parent testing context {t *testing.T} across all test-cases, like in TestGetAccountApiWithFullCoverage
                        or can have separate testing contexts {t *testing.T} across all test-cases, like in TestCreateUserAPI

===============================================================================================================================================
#18  -> Unit Tests with custom gomock matcher

        api-tests for POST requests until now, accept any {gomock.Any()} arg for store methods like CreateUser, CreateAccount
        We should be able to explicitly validate these arg as well in UTs
        These store methods can't be called with "any" arg

        Wrote a custom gomock Matcher for CreateUser api-test to 
                -> validate the arg with which store method is invoked
                -> validate the steps right from request capture till store method invocation
                Note:   took ref from eqMatcher in gomock

===============================================================================================================================================
#19  -> Token Based Authentication - JWT, PASETO

        Summary : PASETO is more secure and updated version of JWT
                  Here you select version, and not alogrithms 
                  It has stronger alogrithms for both local(symmetric-key-en) & public(asymmetric-key-en)
                  Also, algorithm is not specified in header here, prevents attacks, unlike in JWT
                  For local, everything in the token is encrypted and authenticated with a secret key using a strong AEAD - prevents reading as well as tampering, JWT only encodes payload and signs
                  For public, it is similar to JWT, basically encodes the data, and signs with pvt key, but with latest crypto algorithms

        Paseto Token : 
                -> 1st part is paseto verision
                -> 2nd part is purpose of the token - local or public
                -> 3rd part is main content or payload data of the token
                    [local] 
                        It is encrypted with secret-key in case of local, once you decrypt, you get
                        Body                    -> payload body (e.g a data field maybe and an exp time field)
                        Nonce                   -> used in encryption as well as message authentication process
                        Authentication Tag      -> to authenticate the encrypted message and its associated unencrypted data (version, purpose, footer of token)
                    [public] 
                        It is simply encoded, once you decode, there are 2 parts
                        Body                    -> payload body (e.g a data field maybe and an exp time field)
                        Signature               -> signature of the token(signed by issuer's pvt key) which can be verified by the issuer's public key
                -> 4th part (optional) is Footer and you can store any public info in Footer, won't be encrypted but base64 encoded

===============================================================================================================================================
#20  -> JWT & PASETO in Golang

        Create a Maker interface with access token methods - CreateToken & VerifyToken
        Can be Implemented as JWT or PASETO

        Install google's uuid for making each created token unique (even within a user) and store it in Payload struct
        $ go get github.com/google/uuid
        Install go JWT package for implementing Maker with JWT
        $ go get -u github.com/golang-jwt/jwt/v5
        Install go PASETO package for implementing Maker with PASETO
        $ go get -u github.com/o1egl/paseto

        Note:   -> Pay attention to differnt types of error scenarios
                -> An extra JWTPayload struct had to be included along with Payload to adjust for *jwt.RegisteredClaims - must be implemented for jwt payload
                -> Instead of running Payload.Valid like in PASETO VerifyToken, JWT uses inbuilt validation provided by including *jwt.RegisteredClaims 
                -> Use go validator package to validate(the required tag) common Payload struct in both JWT & PASETO

        TODO:   RS256 Signing Algorithm in JWT and UTs for it
        TODO:   write API UTs for user(login)


===============================================================================================================================================
#21  -> Implement authentication middleware and authorization rules in Golang using Gin

        Middleware - checks for auth_header, if present and has expected auth_type and auth_token

        Authorization Rules -
                -> Get User          :       A logged in user can only view user details for himself/herself         :   Refactor API:GetUser

                -> Create Account    :       A logged in user can only create account(s) for himself/herself         :   Refactor API:CreateAccount
                -> Get Account       :       A   logged in user can only get an account that he/she owns               :   Refactor API:GetAccount        
                -> List Accounts     :       A logged in user can only list accounts that belong to him/her          :   Refactor SQL:ListAccounts

                -> Transfer Money    :       A logged in user can only send money from his/her owned account         :   Refactor API:TransferMoney

        TODO:   UTs refactor for all APIs(account,transfer) by adding testcases which include different authorization use-cases
        
===============================================================================================================================================
-------------------------------------------- DOCKER, KUBERNETES, AWS SKIPPED FOR NOW (#22 - #36) --------------------------------------------
===============================================================================================================================================
#37 -> User Session with Refresh Token

        Create new migration for new "sessions" table as expected and run:
        $ make migrateup1
        $ make sqlc
        $ make mock
 
        Modify POST users/login to 
                -> include session creation logic
                -> return created refresh token in response  
        
        Create POST tokens/renew_access to 
                -> issue a new accessToken from refreshToken after all checks pass

        TODO:   write API tests for different scenarios in tokens_test.go 
===============================================================================================================================================
#32 -> Generate db-doc page and schema SQL dump from DBML using dbdocs.io

        STEP-1  https://dbdiagram.io
                Write dbml alongside ER diagram (online) 
                Note:   Put the dbml from STEP-I inside doc/db.dbml file

        STEP-2  https://dbdocs.io
                Install dbdocs cli
                        $ npm i -g dbdocs
                Login via email/github
                        $ dbdocs login
                Generate db docs from dbml (offline) 
                        $ dbdocs build doc/db.dbml
                Secure project with password 
                        $ dbdocs password --set secret --project simple_bank
                Remove project
                        $ dbdocs remove simple_bank

        STEP-3  https://dbml.org 
                Install dbml cli
                        $ npm install -g @dbml/cli
                Generate sql code from dbml (offline) 
                        $ dbml2sql --postgres -o doc/schema.sql doc/db.dbml 
===============================================================================================================================================
