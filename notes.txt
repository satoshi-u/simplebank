===============================================================================================================================================
#1 -> Schema Essentials

        drew the schema here and get simple-bank.pdf/simple-bank.sql : https://dbdiagram.io/d/6443ae366b3194705103b899

===============================================================================================================================================
#2 -> Docker Essentials

        start postgres docker container
        $ docker run --name simple-bank-db -p 5432:5432 -e POSTGRES_USER=root -e POSTGRES_PASSWORD=secret -d postgres:12-alpine

        run cmd inside container (pw not required because image sets up trust authentication locally)
        $ docker exec -it simple-bank-db psql -U root
        $ select now();

        get all containers
        $ docker ps -a

        get logs of container
        $ docker logs simple-bank-db

        stop container
        $ docker stop simple-bank-db

        remove container
        $ docker rm simple-bank-db

        restart container
        $ docker start simple-bank-db

        go inside container
        $ docker exec -it simple-bank-db bash
        $ docker exec -it simple-bank-db /bin/sh

        create a new db with psql cli from inside or directly
        $ createdb --username=root --owner=root simple_bank
        $ docker exec -it simple-bank-db createdb --username=root --owner=root simple_bank

        access new db with psql cli from inside or directly
        $ psql -U root simple_bank
        $ docker exec -it simple-bank-db psql -U root simple_bank

        delete new db with psql cli from inside or directly
        $ dropdb simple_bank
        $ docker exec -it simple-bank-db dropdb simple_bank

        makefile -> easy to setup in local for new folks in project

===============================================================================================================================================
#3 -> DB Migration Essentials

        Install cli golang-migrate
        $ brew install golang-migrate

        create <init_schema> named migration for project with flags
                -seq : uses a sequential number as migration filename's prefix
                -dir : output dir where migration files are saved
        $ migrate create -ext sql -dir db/migration -seq init_schema

        run migrate up/down (included in makefile)
        $ migrate -path db/migration -database "postgresql://root:secret@localhost:5432/simple_bank?sslmode=disable" -verbose up
        $ migrate -path db/migration -database "postgresql://root:secret@localhost:5432/simple_bank?sslmode=disable" -verbose down

===============================================================================================================================================
#4 -> Generate golang CRUD from SQL

        use for doing low-level custom queries : https://pkg.go.dev/database/sql
        use for doing high-level queries : https://pkg.go.dev/gorm.io/gorm
        faster alt : https://pkg.go.dev/github.com/jmoiron/sqlx
        faster, autogen & compile-time alt: https://pkg.go.dev/github.com/kyleconroy/sqlc

        Install sqlc
        $ brew install sqlc

        Init - genrates a yaml with empty settings. For more : https://github.com/kyleconroy/sqlc/tree/v1.4.0
        $ sqlc init

        Init go project and tidy project
        $ go mod init github.com/web3dev6/simplebank
        $ go mod tidy

        Generate code (included in makefile, add .sql in db/query and then run)
        $ sqlc generate

===============================================================================================================================================
#5 -> Golang unit tests for db CRUD

        note*
            $ go get github.com/lib/pq
            Add _ "github.com/lib/pq" in imports for the pgdb driver to work properly
            $ go mod tidy to fix go.mod dependency

        Matching test results in go tests
        $ go get github.com/stretchr/testify

        note*
            Added unit tests for account, entry and transfer db CRUD generated code, which use testQueries conn created in main_test 
        
        $ go test ./...

===============================================================================================================================================
#6 -> SQL DB Txn

        note*
            updated Store to have both Queries and sql.db both
            wrote a generic execTx to carry out SQL DB Txns
            used execTx to carry out multiple create/update queries in one single Tx
            wrote unit test(s) for store 

===============================================================================================================================================
#7 -> DB Txn Lock & dealing with Deadlocks

        note*
            deadlocks happen because of concurrent txns in db
            tx2 : {create transfer}[ok]
            tx2 : {create entry}[ok]
            tx1 : {create transfer}[holding-exclusive-lock]... LOCK HERE
            tx2 : {create entry}[ok]
            tx2 : {get account}[waiting-shared-lock]... WAITING LOCK
            tx1 : {create entry}[ok]
            tx1 : {create entry}[ok]
            tx1 : {get account}[waiting-shared-lock]... DEADLOCK

        note*
            one way to avoid deadlocks is by removing the foreign key constraints on tables[ACCOUNT<->TRANSFER]

            lock is required by "tx1 : {create transfer}" as postgres worries that other txs might update the transfer table's referred from-account-id/to-account-id in accounts table,
            thereby it must guard the foreign key constraint whilst updating the transfer record
            one potential solution is in query "select account for update",
            we must specify that pk won't be touched in "update account" query which is true also
            $ SELECT * FROM accounts WHERE id = $1 LIMIT 1 FOR NO KEY UPDATE;
            Regenerate autogen code sqlc with updated account.sql
            $ make sqlc

            optimize running 2 queries @account{get-update} by having a single query @account{update}

===============================================================================================================================================
#8 -> Avoid Deadlocks - keep order same

        note*
            Deadlock can still happen because of concurrent txns updating pair of same records in different order
            To avoid, make txns update the records in same order always

            Application must acquire locks in a consistent order

===============================================================================================================================================
#9 -> Isolation Levels & Read Phenomenon

        ACID -
                Atomicity   : Either all operations complete successfully or txn fails and db left unchanged
                Consistency : Db state must be valid after the tx, all constraints must be satisfied
                Isolation   : Concurrent txns must not affect each other
                Durability  : Data written by a successful tx must be recorded in persistent storage

        read Phenomenon: -
                Dirty Read            : A txn reads data written by other uncommitted tx
                Non Repeatable Read   : A txn reads the same row twice and sees different value as it got modified by other committed tx
                Phantom Read          : A txn reads a set of rows(based on some condition) twice and sees different values as it got modified by other committed tx
                Serialization Anomaly : The result of a group of concurrent committed txns is impossible to achieve if we run same txns sequentially(no overlaps) in any order
                                        case a. run concurrently 2 times (sum up balance of accounts in table and create new account with that sum as balance)
                                             b. run sequentially 2 times (sum up balance of accounts in table and create new account with that sum as balance)
                                             -> both give different results

        4 std isolation levels -
            Read Uncommitted  <->   doesn't even prevent Dirty Read
            Read Committed    <->   prevents Dirty Read, but Non Repeatable Read & Phantom Read can occur
            Repeatable Read   <->   prevents Dirty Read, Non Repeatable Read & Phantom Read but Serialization Anomaly can occur
            Serializable Read <->   prevents even Serialization Anomaly
        NOTE: rules only applicable for read queries, write is always in sync or throws error
        NOTE: you might wanna have txn retries in place when writes/updates are failed in Repeatable Read/Serializable Read
        NOTE: in postgres, read uncommitted behaves as read committed
        NOTE: in mysql, update to same entry is allowed to happen in concurrent txns with repeatable read mode
        NOTE: in postgres, update to same entry throws error in concurrent txns with repeatable read mode
        NOTE: in mysql's serializable read, serializable anamoly is prevented by throwing deadlock in 2nd txn in concurrent txns affecting same rows
        NOTE: in postgres's serializable read, serializable anamoly is prevented by throwing error in 2nd txn(tells to retry) in concurrent txns affecting same rows

        For an open mysql session,
            get mysql's current isolation level (default is repeatable read)
            $ select @transaction_isolation

            get mysql's global isolation level
            $ select @global.transaction_isolation

            mysql change isolation level
            $ set session transaction isolation level read uncommitted;
            $ set session transaction isolation level read committed;
            $ set session transaction isolation level repeatable read;
            $ set session transaction isolation level serializable read;
        
         For an open postgres session,
            get postgres's current isolation level (default is read committed)
            $ show transaction isolation level

            postgres change isolation level - at txn level
            $ begin
            $ set transaction isolation level read uncommitted;
            $ set transaction isolation level read committed;
            $ set transaction isolation level repeatable read;
            $ set transaction isolation level serializable read;

===============================================================================================================================================
#10 -> CI with Github Actions

        Create your yml file in .github/workflows for running go tests written till now
        For this, you must have the dependencies like golang, golang-migrate-binary, dockerized-postgres
        Then you checkout code, run migrateup and go build/test 
        
        github action for postgres reference
        https://docs.github.com/en/actions/using-containerized-services/creating-postgresql-service-containers

        golang migrate CLI here - find the tar for linux-amd in releases
        https://github.com/golang-migrate/migrate/tree/master/cmd/migrate

===============================================================================================================================================
#11 -> REST with Gin

        Install gin package
        $ go get -u github.com/gin-gonic/gin

        Gin validator package - for validating requests and picking data
        BODY_PARAMS  -> ctx.ShouldBind
        URI_PARAMS   -> ctx.ShouldBindUri
        QUERY_PARAMS -> ctx.ShouldBindQuery

        Gin send reponse -> use ctx.JSON()

        Gin return key-value pairs -> use gin.H (example: in errorResponse)

        TODO:   write api-routes for account(update, delete)

===============================================================================================================================================
#12 -> Viper for configuring project using env-vars/files

        Finds, loads, unmarshalls config from config file
        Supports JSON, YAML, ENV, TOML, INI
       
        Reads config from env-vars/flags
        Set default values, override existing values

        Reads config from remote system, works for both unencrypted and encrypted values 
        Supports Etc, Consul

        Watch for changes in the config file, and notifies application about it
        Re-read changed file, save any modifications made to config file

        Install viper package
        $ go get github.com/spf13/viper
       
        NOTE:   To unmarshall values, viper uses underneath -> $ go get github.com/mitchellh/mapstructure
                So use mapstructure tags to specify name of each config field

        TODO:   config live watching, reading config from remote system

===============================================================================================================================================
#13 -> Test REST APIs with MockDB

        Advantages:
                -> Independent tests : No conflicts with isolated tests data 
                -> Faster tests      : Reduce time spent in connecting/talking to a real db
                -> 100% coverage     : Easily setup edge cases - unexpected errors, connection lost, etc.

        Types of Mock DB:
                -> fake db(memory)   : Implement a fake version of db, store data in memory
                -> db stub(gomock)   : Generate and build stubs that return hard-coded values

        Install mockgo package -> gets bianry in $GOPATH/bin
        $ go install github.com/golang/mock/mockgen@v1.6.0

        To generate mock for Store Interface(lists the TransferTx & Querier methods), use mockgen @<go-module>/<path-to-interface> <interface>
        $ mockgen -destination db/mock/store.go -package mockdb github.com/web3dev6/simplebank/db/sqlc Store
                NOTE: use -destination flag to generate code in a file, default codegen goes to stdout
                NOTE: use -package flag to give a name of your choice for created package

        For API tests:
        Use *mockdb.MockStore created above, build stubs for Get API test-cases, and run these test-requests through a httptest/recorder (dummy server).
        Them evaluate the response as per test-case.

        TODO:   write API tests for all CRUD APIs on Account model

===============================================================================================================================================
#14 -> Transfer API with Custom validation on currency

        using Gin's custom validator:
        https://gin-gonic.com/docs/examples/custom-validators/ 
        which uses:
        https://github.com/go-playground/validator/

        NOTE:   Also wrote in server.go, before starting server, check if any accounts in db.Store 
                If not, create 10 accounts
                *(For this, added GetCountForAccounts in account.sql, and ran 
                        $ make sqlc - to add corresponding method in *Queries and also add in Querier interface while emitting
                        $ make mock - to add corresponding mock method in mockdb's mockStore
                )
        NOTE:  Also currencies-supported now in a central place -> in util.currency.go

        TODO:   write API tests for different scenarios in TransferTx 

===============================================================================================================================================
#15 -> Users table in DB with new migration

        Edit the diagram @ https://dbdiagram.io/d/6443ae366b3194705103b899
                -> to include the users table, and added a FK contraint between username in user-table to owner in account-table
                   now, a user can have multiple accounts
                -> also added a composite unique index for owner and currency in account-table so that a user can have only one account in one currency
                -> download the exported PostgreSQL file
        
        Create <add_user_schema> named migration for project with flags
                -seq : uses a sequential number as migration filename's prefix
                -dir : output dir where migration files are saved
        $ migrate create -ext sql -dir db/migration -seq add_user_schema

        Copy from downloaded .sql file to add_user_schema migration up .sql 
                -> the command to create users table and add FK/UNIQUE constraints
        Copy from downloaded .sql file to add_user_schema migration down .sql 
                -> the command remove FK/UNIQUE constraints, and drop users table

        In Makefile, add migrateup1 & migratedown1 to up/down 1 sequential migration at a time

===============================================================================================================================================
#16 -> sqlc generate for user.sql and handle Db errors in Go

        Run "make sqlc" to create user.sql.go which has create-user & get-user method on *Queries, thereby SQLStore
        Also, these methods were added in the Queries interface
        Now, also run "make mock" for mockgen to add these new methods in MockStore which mocks Store

        Write new user.sql.go db-tests in user_test.go (first write a createRandomUser func)
        Correct db/account.sql.go db-tests in db/account_test.go (must use createRandomUser to create a user, and then use that username as owner in creating acocunt, or else FK constraint fails tests)
        For now, api/account.go api-tests in api/account_test.go don't fail because of FK constraint, because there is no sql db, it's a mock store underneath

===============================================================================================================================================
#17 -> Hash password in Go with Bcrypt

        Added util/password.go and corresponding util/password_test.go
                -> uses golang's bcrypt to hash/check passwords
                -> used this util methods to create password in db/user_test.go and ran UTs successfully

        Added api/user.go to create & get user in users table with proper validations & error handling
        Note:   Hid hashedPassword in response by creating a new struct 

        Add pgx to your Go modules:
        $ go get github.com/jackc/pgx/v5
        Write db errors in a common db/error.go file, to be used in api-tests
                -> ErrRecordNotFound
                -> ErrUniqueViolation 

        Added unit tests for api/user.go in api/user_test.go 
                TestCreateUserAPI :  
                -> OK
                -> InternalError (sql.ErrConnDone)
                -> DuplicateUsername (db.ErrUniqueViolation)
                -> InvalidUsername
                -> InvalidEmail
                -> TooShortPassword
                Note:   func checkResponse :
                        can have the same parent testing context {t *testing.T} across all test-cases, like in TestGetAccountApiWithFullCoverage
                        or can have separate testing contexts {t *testing.T} across all test-cases, like in TestCreateUserAPI

===============================================================================================================================================
#18  -> Unit Tests with custom gomock matcher

        api-tests for POST requests until now, accept any {gomock.Any()} arg for store methods like CreateUser, CreateAccount
        We should be able to explicitly validate these arg as well in UTs
        These store methods can't be called with "any" arg

        Wrote a custom gomock Matcher for CreateUser api-test to 
                -> validate the arg with which store method is invoked
                -> validate the steps right from request capture till store method invocation
                Note:   took ref from eqMatcher in gomock

===============================================================================================================================================
#19  -> Token Based Authentication - JWT, PASETO

        Summary : PASETO is more secure and updated version of JWT
                  Here you select version, and not alogrithms 
                  It has stronger alogrithms for both local(symmetric-key-en) & public(asymmetric-key-en)
                  Also, algorithm is not specified in header here, prevents attacks, unlike in JWT
                  For local, everything in the token is encrypted and authenticated with a secret key using a strong AEAD - prevents reading as well as tampering, JWT only encodes payload and signs
                  For public, it is similar to JWT, basically encodes the data, and signs with pvt key, but with latest crypto algorithms

        Paseto Token : 
                -> 1st part is paseto verision
                -> 2nd part is purpose of the token - local or public
                -> 3rd part is main content or payload data of the token
                    [local] 
                        It is encrypted with secret-key in case of local, once you decrypt, you get
                        Body                    -> payload body (e.g a data field maybe and an exp time field)
                        Nonce                   -> used in encryption as well as message authentication process
                        Authentication Tag      -> to authenticate the encrypted message and its associated unencrypted data (version, purpose, footer of token)
                    [public] 
                        It is simply encoded, once you decode, there are 2 parts
                        Body                    -> payload body (e.g a data field maybe and an exp time field)
                        Signature               -> signature of the token(signed by issuer's pvt key) which can be verified by the issuer's public key
                -> 4th part (optional) is Footer and you can store any public info in Footer, won't be encrypted but base64 encoded

===============================================================================================================================================
#20  -> JWT & PASETO in Golang

        Create a Maker interface with access token methods - CreateToken & VerifyToken
        Can be Implemented as JWT or PASETO

        Install google's uuid for making each created token unique (even within a user) and store it in Payload struct
        $ go get github.com/google/uuid
        Install go JWT package for implementing Maker with JWT
        $ go get -u github.com/golang-jwt/jwt/v5
        Install go PASETO package for implementing Maker with PASETO
        $ go get -u github.com/o1egl/paseto

        Note:   -> Pay attention to differnt types of error scenarios
                -> An extra JWTPayload struct had to be included along with Payload to adjust for *jwt.RegisteredClaims - must be implemented for jwt payload
                -> Instead of running Payload.Valid like in PASETO VerifyToken, JWT uses inbuilt validation provided by including *jwt.RegisteredClaims 
                -> Use go validator package to validate(the required tag) common Payload struct in both JWT & PASETO

        TODO:   RS256 Signing Algorithm in JWT and UTs for it
        TODO:   write API UTs for user(login)


===============================================================================================================================================
#21  -> Implement authentication middleware and authorization rules in Golang using Gin

        Middleware - checks for auth_header, if present and has expected auth_type and auth_token

        Authorization Rules -
                -> Get User          :       A logged in user can only view user details for himself/herself         :   Refactor API:GetUser

                -> Create Account    :       A logged in user can only create account(s) for himself/herself         :   Refactor API:CreateAccount
                -> Get Account       :       A   logged in user can only get an account that he/she owns               :   Refactor API:GetAccount        
                -> List Accounts     :       A logged in user can only list accounts that belong to him/her          :   Refactor SQL:ListAccounts

                -> Transfer Money    :       A logged in user can only send money from his/her owned account         :   Refactor API:TransferMoney

        TODO:   UTs refactor for all APIs(account,transfer) by adding testcases which include different authorization use-cases
        
===============================================================================================================================================
-------------------------------------------- DOCKER, KUBERNETES, AWS SKIPPED FOR NOW (#22 - #36) --------------------------------------------
===============================================================================================================================================
#37 -> User Session with Refresh Token

        Create new migration for new "sessions" table as expected and run:
        $ make migrateup1
        $ make sqlc
        $ make mock
 
        Modify POST users/login to 
                -> include session creation logic
                -> return created refresh token in response  
        
        Create POST tokens/renew_access to 
                -> issue a new accessToken from refreshToken after all checks pass

        TODO:   write API tests for different scenarios in tokens_test.go 
===============================================================================================================================================
#38 -> Generate db-doc page and schema SQL dump from DBML using dbdocs.io

        STEP-1  https://dbdiagram.io
                Write dbml alongside ER diagram (online) 
                Note:   Put the dbml from STEP-I inside doc/db.dbml file

        STEP-2  https://dbdocs.io
                Install dbdocs cli
                        $ npm i -g dbdocs
                Login via email/github
                        $ dbdocs login
                Generate db docs from dbml (offline) 
                        $ dbdocs build doc/db.dbml
                Secure project with password 
                        $ dbdocs password --set secret --project simple_bank
                Remove project
                        $ dbdocs remove simple_bank

        STEP-3  https://dbml.org 
                Install dbml cli
                        $ npm install -g @dbml/cli
                Generate sql code from dbml (offline) 
                        $ dbml2sql --postgres -o doc/schema.sql doc/db.dbml 
===============================================================================================================================================
#39 -> Intro to GRPC  

        Basics:
                The client can execute a remote procedure on the server 
                        -> Can call it as if the server function is in client local codebase
                The remote interaction code and all that stuff
                        -> Handled by gRPC
                The RPC APIs and their data structure code is automatically generated from Api-definition
                        -> Using a special program called protocol buffer compiler (protobuf)
                Supports multiple programming languages  
                        -> From the same API-definition(proto), both server & client code can be generated 

        Steps:
                First, define RPC API(Service) & its Request-Response(Message) structure using Protobuf in a .proto file
                Second, generate gRPC stubs in the language of your choice using protobuf compiler
                Third, server side, implement the RPC handler
                Fourth, client side, use the generated client stubs to make RPC calls to   server 
          
        Pros:
                -> High performance with http/2 protocol
                        transferring data in binary format
                        multiplexing, which allows sending multiple requests through the same TCP connection
                        header-compression 
                        bi-directional communication between client and server
                -> Strong API contract
                        server and client, both share the same protobuf defintion of the API(RPC)
                        where, the request-response data strcutures are strongly typed
                -> Automatic code generation
                        codes that serialize/deserialize data or transfer data between client & server are automaticlly generated
                        the code can be in multiple languages

        Types: 
                -> Unary gRPC 
                        client sends one single request, 
                        server replies with one single response, 
                        just like an http api 
                -> Client streaming gRPC
                        client sends a stream of multiple messages,
                        server sends back one single response
                -> Server streaming gRPC
                        client sends a one single request, 
                        server replies with a stream of multiple messages
                -> Bi-directional streaming gRPC
                        most complex,
                        client and server will keep sending and receiving multiple messages,
                        in parallel and with arbitrary order,
                        flexible and no blocking, so no side needs to wait for the response from the other before sending the next messages  
        
         GRPC Gateway:
                -> What is grpc gateway:
                        Write server code once,
                        Serve both grpc and http/json requests at the same time
                        It is a plugin of protobuf that genrate HTTP proxy codes from protobuf definition
                        So from protobuf codegen for client, both code for grpc-handling as well as http-handling is generated
                        A normal grpc client will connec t directly to the grpc server to send a grpc request and receive a binary response
                        An http client will connect to the gateway server to send an http request, 
                        This http request is translated into grpc formated and forwarded to grpc service handler to be processed
                        Also, the grpc response is translated back into json format before returning to the http client

                -> Two types of translation:
                        1. In-process translation 
                                gateway can call the grpc handler directly in code, with no extra network hop
                                works only for unary grpc
                        2. Separate proxy server
                                the http json request here will be trnaslated and forwarded to the grpc server via a network call                          
===============================================================================================================================================        
#40 -> Define gRPC API and go-codegen with protobuf

        Quickstart Go https://grpc.io/docs/languages/go/quickstart/

        Install protoc:
        $ brew install protobuf
 
        Install the protocol compiler plugins for Go using the following commands:
        $ go install google.golang.org/protobuf/cmd/protoc-gen-go@v1.28
        $ go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@v1.2  

        api - write grpc api for create-user and login-user 

        NOTE:
                Message types(scalar) : https://protobuf.dev/programming-guides/proto3/#scalar 
                
                Field Numbers used to uniquely serialize/deserialize fields in message, numbers from 1 to 15 take only 1 byte to encode - smaller message size
                
                For resolving relative import errors, tell in protoc settings.json, to look in path "protos" instead of default "."
                
                Service SimpleBank is a unary gRPC service - write all protos in .proto files
                
                Codegen for go with  :
                $ protoc --go_out=. --go_opt=paths=source_relative \
                  --go-grpc_out=. --go-grpc_opt=paths=source_relative \
                  helloworld/helloworld.proto  
                This generates corresponding .go files for .proto files, and one extra service_simple_bank_grpc.pb.go file, 
                This file contains grpc server and client intefaces or stubs, based on which, we write our real implementaion
                *** Use generated code to implemet the gRPC API service handler on the server side

                go mod tidy - to download missing packages - google.golang.org/grpc 

===============================================================================================================================================
#41 -> Run a golang gRPC server and call its API with evans

        Create a new gapi package to redo the Simple_Bank APIs the gRPC way
        Create a new struct for our server with pb.UnimplementedSimpleBankServer
        In main.go, separate out http/grpc with runGinServer & runGrpcServer and a config to start only one of them

        Now, use evans to test the unimplemented simple-bank grpc server
        $ brew tap ktr0731/evans
        $ brew install evans
        
        Evans by default dials tcp 127.0.0.1:50051 where our listner port is 9090, use below to start evans console:
        $ evans --host localhost --port 9090 -r repl
        cmds:
        $ show service
        $ call CreateUser

===============================================================================================================================================
#42 -> Implement gRPC APIs to create and login users

        Create these APIs in separate files in gapi package
        Reuse logic from Gin APIs
        Note:   There is differnce between db.User and pb.User - write utils to convert if too much reuse in code
                Use timestamppb.New() to create google.protobuf.Timestamp in pb.User from time.Timestamp in db.User
                For now, let UserAgent and ClientIp be empty while creating session in LoginUser gRPC API (no gin context to get this info)
===============================================================================================================================================
#43 -> gRPC Gateway - serve both gRPC & HTTP requests

        1. Refer grpc-gateway https://github.com/grpc-ecosystem/grpc-gateway

        2. Add the following in your project :-
        {{{{{
                // +build tools

                package tools

                import (
                _ "github.com/grpc-ecosystem/grpc-gateway/v2/protoc-gen-grpc-gateway"
                _ "github.com/grpc-ecosystem/grpc-gateway/v2/protoc-gen-openapiv2"
                _ "google.golang.org/grpc/cmd/protoc-gen-go-grpc"
                _ "google.go lang.org/protobuf/cmd/protoc-gen-go"
                )
        }}}}}

       3. Install all binaries of the plugins to $GOBIN, used by protoc to generate code :-
                $ go install \
                github.com/grpc-ecosystem/grpc-gateway/v2/protoc-gen-grpc-gateway \
                github.com/grpc-ecosystem/grpc-gateway/v2/protoc-gen-openapiv2 \
                google.golang.org/protobuf/cmd/protoc-gen-go \
                google.golang.org/grpc/cmd/protoc-gen-go-grpc

        Note:   You need not use 'buf generate' to genrate go files, as you have already done that using protoc directly
                You need to use only protoc-gen-grpc-gateway binary to generate the grpc gateway go code

        4. We will use the 2nd option of generating grpc gateway by .proto modifications to use custom http mappings - With custom annotations
           Add a google.api.http annotation to your .proto file
                option (google.api.http) = {
                        post: "/v1/example/echo"
                        body: "*"
                };
           If you are using protoc to generate stubs, 
           you need to ensure the required dependencies 
           are available to the compiler at compile time. 
           These can be found by manually cloning and copying 
           the relevant files from the googleapis repository, 
           and providing them to protoc when running. 
           The files you will need are:
                google/api/annotations.proto
                google/api/field_behavior.proto
                google/api/http.proto
                google/api/httpbody.proto
           In your project, create dir proto/google/api, then separately clone https://github.com/googleapis/googleapis, cd into it and copy files :
                $ cp google/api/annotations.proto ../simplebank/proto/google/api
                $ cp google/api/field_behavior.proto ../simplebank/proto/google/api
                $ cp google/api/http.proto ../simplebank/proto/google/api
                $ cp google/api/httpbody.proto ../simplebank/proto/google/api

        5. Now add the google.api.http annotation with correct api paths and versioning,
           for both create_user and login_user

        6. Update the protoc in Makefile to generate both grpc-stubs and grpc-gateway
           Add the follwing:-
           $ --grpc-gateway_out=pb --grpc-  gateway_opt paths=source_relative \
           To know more:-
           $ protoc-gen-grpc-gateway --help
           After these changes, run:-
           $ make proto
        
        7. Finally, in main.go, write `func runGatewayServer()`, to run grpc-gateway (in-process-translation method),
           which runs along with `func runGrpcServer()`.
           Configure all 3 modes of server separately via app.env as HTTP, GRPC, GRPC_GATEWAY

        8. The response from grpc's http-gateway is camel-case, make it snake-case
           https://grpc-ecosystem.github.io/grpc-gateway/docs/mapping/customizing_your_gateway/#using-proto-names-in-json


        Note:   -> Evans cannot run directly with default package and service using 
                        google.golang.org/grpc/cmd/protoc-gen-go-grpc v1.3.0 
                   It worked fine previously when we used
                        google.golang.org/grpc/cmd/protoc-gen-go-grpc v1.3.0 
                   Once you run `make evans`, make sure to also run
                        $ package pb
                        $ service SimpleBank     
                
        TODO:   -> Write GRPC equivalent of all Gin Apis(package api - user, account, token, transfer) in gapi package
  
===============================================================================================================================================
#44 -> Extract info from gRPC metadata

        Using below packages to get metadata like ClientIp & UserAgent from the grpc context
        "google.golang.org/grpc/metadata"
	"google.golang.org/grpc/peer"

        Note:   -> There has to be multiple if-blocks in ExtractMetadata() for differnt header-names of differnt clients

===============================================================================================================================================
#45 -> Automatic   & serve Swagger docs for Go grpc-gateway server

        1.
        Modify the `make proto` in Makefile by adding :-
                rm -f doc/swagger/*.swagger.json
                --openapiv2_out=doc/swagger --openapiv2_opt=allow_merge=true,merge_file_name=simple_bank \
        Note:   -> You already have the open-api dependency "github.com/grpc-ecosystem/grpc-gateway/v2/protoc-gen-openapiv2" in your tools package


        2.
        Now, let's try to customize our swagger.json content with open-api options, modify service_simple_bank.proto with :-
import "protoc-gen-openapiv2/options/annotations.proto";
option (grpc.gateway.protoc_gen_openapiv2.options.openapiv2_swagger) = {
  info: {
    title: "Simple Bank API";
    version: "1.0";
    contact: {
      name: "web3dev6";
      url: "https://github.com/web3dev6";
      email: "sarthakjoshi.in@gmail.com@example.com";
    };
  };
};
        For the open-api import to work in your project,
        the external proto files we will need are:
                protoc-gen-openapiv2/options/annotations.proto
                protoc-gen-openapiv2/options/openapiv2.proto
        In your project dir proto, create protoc-gen-openapiv2/options dir, run:-
                $ mkdir -p proto/protoc-gen-openapiv2/options
        Then separately clone https://github.com/grpc-ecosystem/grpc-gateway.git, cd into it and copy files, run:-
                $ cp protoc-gen-openapiv2/options/annotations.proto ../simplebank/proto/protoc-gen-openapiv2/options
                $ cp protoc-gen-openapiv2/options/openapiv2.proto ../simplebank/proto/protoc-gen-openapiv2/options
         Note:   -> (ONE-LINER) 
                $ cp protoc-gen-openapiv2/options/*.proto ../simplebank/proto/protoc-gen-openapiv2/options
  

        3.
        Use swagger-ui to dynamically generate beautiful documentation from a Swagger-compliant API (our swagger.json file)
        Refer https://github.com/swagger-api/swagger-ui

        We will use the 2nd option:-
        swagger-ui-dist 
        It is a dependency-free module that includes everything you need to serve Swagger UI in
        a server-side project, 
        or a single-page application that can't resolve npm module dependencies.

        Clone the repo https://github.com/swagger-api/swagger-ui.git,
        and copy the HTML, JavaScript, and CSS assets required, into doc/swagger dir, along with swagger.json file, run:-
                $ cp -r dist/* ../simplebank/doc/swagger
        Now, modify the url in swagger-initializer.js to:-
                url: "simple_bank.swagger.json"
        
        Serve the whole doc/swagger dir as static resources from your main.go in func runGatewayServer()
        Note:   -> Use StripPrefix to strip the route prefix of the url before passing the request to the static file server
                -> In `make proto` of Makefile, in openapiv2_opt, add json_names_for_fields=false -> to get swagger doc req/res fields with snake-case
        
        TODO:   -> Gin Server & Swagger Docs, same as GRPC Gateway Docs
===============================================================================================================================================
#46 -> Embed static frontend files inside Golang backend server's binary using statik

        Simply, add `github.com/rakyll/statik` in your tools package and run go mod tidy
        Also, Run to install the statik-cli:-
        $ go get github.com/rakyll/statik

        Update `make proto` in Makefile, to generate static binary package -> everytime we regenerate the swager doc, add :-
                statik -src=./doc/swagger -dest=./doc
        Next, run `make proto`
        Next, instead of http-fs, create and use the statik-fs to   server swagger doc

        Note:   -> error  : cannot create statik fs:statik/fs: no zip data registered
                   reason : the generated statik package was never imported/loaded, init func was never called, no data registered in zipData
                   fix    : add the following blank import for the statik sub-package at top of main.go :-
                            _ "github.com/web3dev6/simplebank/doc/statik"
                   check  : go to http://localhost:8080/swagger

        [Bonus] Let's add summary and desc for each of our swagger API, by modifying proto/service_simple_bank.proto, add:-
        option (grpc.gateway.protoc_gen_openapiv2.options.openapiv2_operation) = {
          description: "Use this API to create a new user";
          summary: "Create new user";
        };
        (ref:- https://github.com/grpc-ecosystem/grpc-gateway/blob/main/examples/internal/proto/examplepb/a_bit_of_everything.proto)

===============================================================================================================================================
#47 -> Validate gRPC parameters and send human/machine friendly response

        In gin validations, it used `github.com/go-playground/validator/v10` internally to handle field validations of client requests via struct tags.
        
        In grpc validations, we can write our own functions to handle the validations in validator.go, 
        and use them in gapi - CreateUser, LoginUser etc,
        along with proper error handling,
        returning bad responses with proper error code, msg and WithDetails

        TODO:   -> Write validations for LoginUser gapi
                -> Improve gin api responses with proper error formatting like in gapi

===============================================================================================================================================
#48 -> Run DB migrations directly inside Golang code

        Add this package in main.go, and run go mod tidy:-
        "github.com/golang-migrate/migrate/v4" 

        Add code to run migration by pointing to migration dir and dbSource.

        Get error :-
        cannot create new migrate instance:source driver: unknown driver 'file' (forgotten import?)
        cannot create new migrate instance:database driver: unknown driver postgresql (forgotten import?)

        Fix error by addi g blank imports of drivers:-
        _ "github.com/golang-migrate/migrate/v4/source/file"
        _ "github.com/golang-migrate/migrate/v4/database/postgres"

        Note:   -> when no new migration, ignore the error in migrate.Up()

===============================================================================================================================================
#49 -> Parital update in sqlc, working with nullable fields

APPROACH-I

        1. add UPDATE query for a user in db/query/user.sql:-
-- name: UpdateUser :one
UPDATE users
SET hashed_password = $1,
    full_name = $2,
    email = $3
WHERE username = $4
RETURNING *;

        2. modify it such that it allows optional field updates:-
-- name: UpdateUser :one
UPDATE users
SET hashed_password = CASE
        WHEN $1 = TRUE THEN $2
        ELSE hashed_password
    full_name = CASE
        WHEN $3 = TRUE THEN $4
        ELSE full_name
    email = CASE
        WHEN $5 = TRUE THEN $6
        ELSE email
WHERE username = $7
RETURNING *;

        3. modify it further such that the generated UpdateUserParams struct has better names/types for boolean fields:- 
        Note:   @ is equivalent to sqlc.arg
        Note:   cannot mix positional parameters or named parameters or @ parameters
-- name: UpdateUser :one
UPDATE users
SET hashed_password = CASE 
        WHEN @set_hashed_password::boolean = TRUE THEN @hashed_password
        ELSE hashed_password
    END,
    full_name = CASE
         WHEN @set_full_name::boolean = TRUE THEN @full_name
        ELSE full_name
    END,
    email = CASE
         WHEN @set_email::boolean = TRUE THEN @email
        ELSE email
    END 
WHERE username = @username
RETURNING *;
         
APPROACH-II

        Nullable Parameters with COALESCE & sqlc.narg
-- name: GetCountForUsers :one
SELECT COUNT(*) FROM users;
-- name: UpdateUser :one
UPDATE users
SET hashed_password = COALESCE(sqlc.narg(hashed_password), hashed_password),
    full_name = COALESCE(sqlc.narg(full_name), full_name),
    email = COALESCE(sqlc.narg(email), email)
WHERE username = sqlc.arg(username)
RETURNING *;
===============================================================================================================================================
#50 -> Build gRPC update API with optional/nullable parameters

        Iterating on #49, where we generated db/sqlc code to update a user,
        Here let's write a grpc endpoint to take user inputs to update user in db
                1. Write a new proto file for the endpoint in protos dir
                2. Use optional keyword in message UpdateUserRequest to make fields optional in generated code(*string instead of string) 
                3. In gapi, create a file for update_user rpc, and craft UpdateUserParams parmas carefully
                4. In func validateUpdateUserRequest, validate only if not nil param
        Test with insomnia for all test-cases

        TODO:   -> Write gin endpoint for update_user

===============================================================================================================================================
#51 -> Add authorization to protect gRPC API

        Write func authorizeUser where bearer token is fetched form metadata with auth_header name
        Check for token type as bearer before getting payload with VerifyToken()
        Call this authorizeUser in gapi handler - it makes things work for both grpc and http-gateway server, unlike a gprc interceptor
        Ensure proper error handling with code reusability
        Fixed gin update_user -> added token authentication & authorization to it, and added proper nil checks to make update_fields optional without panics with *string

        TODO:   -> Implement a gprc interceptor

===============================================================================================================================================
#52 -> Write structured logs for gRPC APIs using grpc interceptor

        Info needed in logs:
                1. what gRPC method called ?  
                2. how long for request to be processed ?
                3. what the response status code was ?
                4. json structured logs - easily parsed and indexed by  tools like Logstash, fluentd, Grafana loki

        We would be using https://github.com/rs/zerolog for writing our structured logs.
                $ go get -u github.com/rs/zerolog/log
        Easily write logs in json format with zero allocation.
        Configure zerolog initially in main.go as per ENV - json if production, colourful if development

        Note:   -> No logs appear when we send http request to grpc-gateway server
                   That is because we are using in-process translation on our gRPC gateway server
                   So the gateway will directly call the RPC handler function, without going thru any grpc interceptor(our logger module)
                   If we run the gateway as a separate server, and use cross-process translation to call our gRPC server via network call, logs will show up in gRPC server like before
                   But this introduces another network hop, can increase the duration of the request
                   So if we still want to keep using in-process translation, we will have to write a separate HTTP middleware to log the HTTP requests

===============================================================================================================================================
#53 ->  Write HTTP logger middleware in Go

        1. Write a separate HTTP middleware in looger.go to log the HTTP requests
        2. Then in main.go, attach logger middleware handler in mux handler context
        3. Now when http.Serve is called, call it with logger middleware handler instead of mux handler directly

        Note:   -> In HttpLogger's http.HandlerFunc, call ServeHTTP func with custom ResponseRecorder(has embedded ResponseWriter) instead of http.ResponseWriter directly
                   It helps to capture status code from handler response and log it
                   ResponseRecorder also attaches error in logs, if response doesn't have http.StatusOK

        TODO:   -> Write gin http-logger using zerolog

===============================================================================================================================================
#54 -> Implement background worker in Go with Redis and asynq

      Use case: send verification email (gRPC : for package gapi)

        1. Create a new user record in databse  
        2. Push a send verification email task to the Redis queue 
        3. Background worker picks up a task from the queue and process it 

        We will be using asynq- distributed task queue lib: https://github.com/hibiken/asynq
        A worker goroutine is started for each task in queue,
        So tasks are processed concurrently by multiple workers. 

          
